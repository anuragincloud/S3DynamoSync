import json
import urllib.parse
import boto3
import csv

# Initialize S3 and DynamoDB resources
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

# Reference the DynamoDB table
inventory_table = dynamodb.Table('Inventory')

# Lambda handler function
def lambda_handler(event, context):
    try:
        # Log the incoming event
        print("Event received by Lambda function: " + json.dumps(event, indent=2))
        
        # Extract bucket name and object key from the event
        bucket = event['Records'][0]['s3']['bucket']['name']
        key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'])
        local_filename = '/tmp/inventory.txt'

        # Download the file from S3
        s3.download_file(bucket, key, local_filename)
        
        # Process the downloaded file
        with open(local_filename, 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            row_count = 0
            
            for row in reader:
                row_count += 1
                print(f"Processing row: store={row['store']}, item={row['item']}, count={row['count']}")
                
                # Insert data into DynamoDB
                inventory_table.put_item(
                    Item={
                        'store': row['store'],
                        'item': row['item'],
                        'count': int(row['count'])
                    }
                )
                
            print(f"Successfully inserted {row_count} records into DynamoDB.")
            return {
                'statusCode': 200,
                'body': json.dumps(f"{row_count} records inserted")
            }
    
    except Exception as e:
        print(f"Error: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps(f"Error processing file: {str(e)}")
        }
